<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->
<a name="readme-top"></a>

<br />
<div align="center">
  <a href="https://github.com/Isa1asN/local-rag">
    <div style="background-color: white;">
      <img src="images/logoimg.png" alt="Logo">
    </div>
  </a>

<h1 align="center">Local RAG: Langchain + Ollama + Streamlit </h1>

  <p align="center">
    Build your own RAG (chatPDF) and run it locally.
  </p>
  <img src="https://github.com/Isa1asN/local-rag/blob/master/images/screenshot.png?raw=true" alt="sshot">

</div>

Setups:
- install ollama from https://ollama.ai/ 
- pull the llama3:latest model on ollama using the command `ollama pull llama3:latest`
- install the requirements using `pip install -r requirements.txt`
- run the streamlit app using `streamlit run main.py`
